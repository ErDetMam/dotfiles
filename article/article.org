#+startup: beamer
#+TITLE:     Dynamic Conditional Random Fields
#+SUBTITLE: Factorized Probabilistic Models for Labeling and Segmenting Data
#+AUTHOR:    Martin Zimmer Kristensen
# #+DATE:      2016-30-10
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t texht:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [presentation,bigger]
# #+latex_header: \usepackage{beamerarticle}
# #+latex_header: \usepackage{amsmath}
#+latex_header: \mode<beamer>{\usetheme{Madrid}}
#+OPTIONS: H:2 TOC:1
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
* Introduction
** Introduction
   - Sequential Data
   - Generative Versus Discriminative
   - Conditional Random Fields
   # #+ATTR_LATEX: :float t :width 5cm :center t
   # [[/home/martin/article/figures/LCRF.pdf]]
** Sequential Data
# something about error cascading
*** Part-of-speech Tagging
    The *[DT]* little *[JJ]* dog *[NN]* was *[VBD]* furious *[JJ]* and *[CC]* barked *[VBD]* at *[IN]* the *[DT]* large *[JJ]* human *[NN]*
*** Noun-phrase Chunking
    The *[B-NP]* little *[I-NP]* dog *[I-NP]* was *[O]* furious *[O]* and *[O]* barked *[O]* at *[O]* the *[B-NP]* large *[I-NP]* human *[I-NP]*
** Sequential Data
*** Other
    - Named Entity Recognition
    - Speech Recognition
** Generative Versus Discriminative
*** Generative Models:
    - The joint probability $p(x,y)$
      - Able to generate $x$
      - Assumptions to achieve tractability:
        - Naive Bayes assumption
      - Modeling interdependent features is difficult
** Generative Versus Discriminative
*** Discriminative Models:
    - The conditional probability: $p(y|x)$
      - Assumptions among $y$
      - Assumptions among $y$ and $x$
      - Interdependent features
        - Capitalization, prefixes, suffixes, neighboring words...
      - Unseen words can be labeled by using their features
# ** Generative Versus Discriminative
#    #+ATTR_LATEX: :float t :width \textwidth :center t
#    [[file:figures/discgen.pdf]]
# ** Conditional Random Fields (CRF)
#    #+ATTR_LATEX: :float t :width 8cm :center t
#    [[file:figures/LCRF.pdf]]
** Conditional Random Fields (CRF)
*** CRF                                                        :B_definition:
    :PROPERTIES:
    :BEAMER_env: definition
    :END:
    - Let $G$ be an undirected model over sets of random variables $y$ and $x$
    - Let $C = \{\{y_c, x_c\}\}$ be the set of cliques in $G$
    - Conditional probability defined as:
   \[ p_\Lambda(y|x) = \dfrac{1}{Z(x)}\prod_{c \in C} \Phi (y_c, x_c) \]
   - $\Phi$ is a potential function
   - $Z(x)$ is a normalization factor
** Potential function
*** Feature Functions
   - Potentials factorize according to a set of features functions $\{f_k\}$:
   \[ f(y_c,x_c) = exp\Bigg(\sum_k \lambda_kf_k(y_c,x_c)\Bigg) \]
** Linear-chain CRF
#+ATTR_LATEX: :float t :width 5cm :center t
[[/home/martin/article/figures/LCRF.pdf]]
   - A special case of CRFs where the first-order Markov assumption is made over the latent variables.
   - Then the feature functions can be described as:
    \[ f_k(y_{t-1},y_t,x,t) \]
** Linear-chain CRF
*** Feature Functions:
    - $f(y_{t-1}, y_t, x, t) = 1$:
      - /iff/ $y_{t-1} = adjective$, $y_t = \textit{proper noun}$, and $x_t$ begins with a capital letter.
    - $f(y_{t-1}, y_t, x, t) = 1$:
      - /iff/ $y_t = \textit{organization}$, $x_{t} = \textit{``New''}$, $x_{t+1} = \textit{``York''}$, and $x_{t+2} = \textit{``Times''}$
* Dynamic Conditional Random Fields
** Key Contributions
   - Dynamic Conditional Random Fields (DCRF)
     - Factorial CRF
     - Exact inference for some models
     - Inference approximation:
       - Lower training time
       - Equal performance
   #+attr_latex: :float t :width \textwidth :center t
  [[file:figures/DCRF.pdf]]
# ** DCRF
#    - Let $y = \{y_1 \dots{} y_T\}$ be a sequence of random vectors:
#      - $y_i = (y_{i1} \ldots{} y_{im})$ where $y_i$ is the state vector at time $i$, and
#      - $y_{ij}$ is the value of variable $j$ at time $t$
# ** Clique Index
#    - To give the likelihood equation, we require a way to describe a clique independent of its position in the sequence:
# *** Clique Index                                               :B_definition:
#     :PROPERTIES:
#     :BEAMER_env: definition
#     :END:
#     - Given a time $t$, denote any variable $y_{ij}$ in $y$ by:
#       - Its index $j$ in $y_i$
#       - Its time offset $\Delta t = i-t$
#     - Then $c = \{(\Delta t, j)\}$ is a clique index, which denotes a set of variables $y_{t,c} =uiv \{y_{t+\Delta t,j} | (\Delta t,j) \in c\}$
** DCRF
*** Dynamic Conditional Random Field                           :B_definition:
    :PROPERTIES:
    :BEAMER_env: definition
    :END:
    - Cliques are defined by its index $j$ and its time offset $\Delta t = i-t$
      - I.e. $y_{12}$ when $t = 1$ is denoted as $y_{02}$ since $\Delta t = 0$
    - $p(y|x) = \dfrac{1}{Z(x)}\displaystyle \prod_{t}\prod_{c \in C} \text{exp}\Bigg(\sum_k \lambda_k f_k(y_{t,c},x,t)\Bigg)$
    - where $Z(x)$ is the partition function
** Factorial CRF
   - A DCRF which has linear chains of labels, with connection between cotemporal labels.
   #+attr_latex: :float t :width 5cm :center t
   [[file:figures/FCRF.pdf]]
** Factorial CRF
*** Cliques
    - The cliques are of the form:
      - Within-chain edges: \text{ }\text{ }$\{(0,\ell),(1,\ell)\}$
      - Between-chain edges: $\{(0,\ell),(0,\ell+1)\}$
** Factorial CRF
*** Factorial CRF                                              :B_definition:
    :PROPERTIES:
    :BEAMER_env: definition
    :END:
    $p(x|y) = \dfrac{1}{Z(x)}\Bigg(\displaystyle\prod_{t=1}^{T-1}\prod_{\ell=1}^{L}\Phi_\ell(y_{\ell,t},y_{\ell,t+1},x,t)\Bigg)\Bigg(\prod_{t=1}^{T}\prod_{\ell=1}^{L-1}\Psi_\ell(y_{\ell,t},y_{\ell+1,t},x,t)\Bigg)$
    - $\{\Phi_\ell\}$ are the factors over within-chain edges
    - $\{\Psi_\ell\}$ are the factors over between-chain edges
    - $Z(x)$ is the partition function.
** Factorial CRF
*** Factors
     - The factors are modeled using features $\{f_k\}$ and weights $\{\lambda_k\}$ of $G$ as:
       \[\Phi_\ell(y_{\ell,t},y_{\ell,t+1},x,t) = \text{exp}\Bigg\{\sum_k\lambda_k f_k(y_{\ell,t},y_{\ell,t+1},x,t)\Bigg\}\text{,}\]
       \[\Psi_\ell(y_{\ell,t},y_{\ell+1,t},x,t) = \text{exp}\Bigg\{\sum_k\lambda_k f_k(y_{\ell,t},y_{\ell+1,t},x,t)\Bigg\}\text{.}\]
** Inference
   - Exact inference can be expensive for many models
   - Use approximate inference using loopy belief propagation
** Inference
*** Loopy Belief Propagation
    - Message from node $x_u$ to node $x_v$:
      \[ m_{x_u}(x_v) \]
    - Value of $m_{x_u}(x_v)$:
      - The belief of $x_u$ about the probability $p(x_j)$
    - Iteratively send messages until convergence
    - Different schedules can be applied
      - Random
      - Tree-based (send messages from leaves to root and back)
** Parameter Estimation
   - Given training data $D = \{x^{(i)},y^{(i)}\}^N_{i=1}$
     - Finding a set of parameters $\Lambda = \{\lambda_k\}$
   - Assign weights $\lambda_k$ such that we are accurate on the training data.
* Experiments
** Experiments
*** Noun-phrase Chunking
    The *[B-NP]* little *[I-NP]* dog *[I-NP]* was *[O]* furious *[O]* and *[O]* barked *[O]* at *[O]* the *[B-NP]* large *[I-NP]* human *[I-NP]*
*** Usual approach:
    1. POS tagging
    2. Noun-phrase Chunking
*** Challenge:
    - Mistakes in POS tagging will cascade onto noun-phrase chunking
** Experiments
*** Data:
    - CoNLL 2000
*** Approach:
   - Use a factorial CRF to jointly do POS and chunking
*** Compare to:
    - CRF+CRF
    - Brill+CRF
      - Brill tagger trained on over four times more data including the CoNLL 2000
** Results
 #+ATTR_LATEX: :float t :width 8cm :center t
  [[file:figures/npgraph.pdf]]
** Results
 #+ATTR_LATEX: :float t :width 7cm :center t
  [[file:figures/nptab.pdf]]
** Inference Algorithms
 #+ATTR_LATEX: :float t :width \textwidth :center t
  [[file:figures/npinf.pdf]]
* Conclusions
** Conclusions
   - Factorial CRFs are useful for NP tasks
   - Loopy belief propagation:
     - Performs equally to exact inference
     - Reduces training time

     
